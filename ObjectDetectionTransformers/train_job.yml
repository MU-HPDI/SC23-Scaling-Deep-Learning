# batch/v1 tells it to use the JOB API
apiVersion: batch/v1
# we are running a Job, not a Pod
kind: Job

# set the name of the job
metadata:
  name: train-$MODEL-$DATASET

spec:
  # how many times should the system
  # retry before calling it a failure
  backoffLimit: 0
  template:
    spec:
      # should we restart on failure
      restartPolicy: Never
      # what containers will we need
      containers:
        # the name of the container
        - name: mmdet
          # the image: can be from any pubic facing registry
          image: jalexhurt/sc23-object-detection-transformers:latest
          # the working dir when the container starts
          workingDir: $WORKDIR
          # should Kube pull it
          imagePullPolicy: IfNotPresent
          # we need to expose the port
          # that will be used for DDP
          ports:
            - containerPort: 8880
          # setting of env variables
          env:
            # which GPUs
            - name: MMDET_GPU_IDS
              value: all
            # which dir to output to
            - name: MMDET_WORK_DIR
              value: $WORKDIR/train_output
            # the batch size
            - name: MMDET_BATCH_SIZE
              value: "$BATCHSIZE"
            # which interface to use
            - name: NCCL_SOCKET_IFNAME
              value: eth0
            # prints some INFO level
            # NCCL logs
            - name: NCCL_DEBUG
              value: INFO
          # the command to run when the container starts
          command:
            ["python", "-m", "cgimmd", "train", "./train_cfg.py"]
          # define the resources for this container
          resources:
            # limits - the max given to the container
            limits:
              # RAM
              memory: 64Gi
              # cores
              cpu: 16
              # NVIDIA GPUs
              nvidia.com/gpu: 4
            # requests - what we'd like
            requests:
              # RAM
              memory: 8Gi
              # CPU Cores
              cpu: 6
              # GPUs
              nvidia.com/gpu: 4
          # what volumes should we mount
          volumeMounts:
            # my datasets PVC should mount to /data
            - mountPath: /data
              name: jhurt-data
            # my experiments and results should mount to /output
            - mountPath: /output
              name: jhurt-experiments
            # IMPORTANT: we need SHM for DDP
            - mountPath: /dev/shm
              name: dshm
      # tell Kube where to find the volumes we want to use
      volumes:
        # which PVC is my data
        - name: jhurt-data
          persistentVolumeClaim:
            claimName: jhurt-data
        # which PVC is my experiments
        - name: jhurt-experiments
          persistentVolumeClaim:
            claimName: jhurt-experiments
        # setup shared memory as a RAM volume
        - name: dshm
          emptyDir:
            medium: Memory